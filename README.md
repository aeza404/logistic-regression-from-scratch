# logistic-regression-from-scratch
This lab focuses on the modeling stage of the machine learning life cycle, where the goal is to deepen understanding of logistic regression by implementing it entirely from scratch. Instead of relying on machine learning libraries, you build the mathematical operations behind logistic regression manually, including the inverse-logit function, the gradient of the log-loss, and the Hessian matrix used for optimization. You then integrate these components into a Python class, LogisticRegressionScratch, that computes predictions, updates weights using Newton’s method, and determines when the training process has converged.

After implementing the class, the lab shifts to applying it to a real dataset. You load a preprocessed Airbnb dataset and define the prediction task: determining whether a host is a "superhost" based on a selection of numerical review scores and host performance metrics. The chosen features are extracted into a feature matrix X, while the label column forms the target vector y. With these, you train your custom logistic regression model and examine the learned weights and intercept to understand how different features influence the likelihood of a host being classified as a superhost.

The final phase of the lab compares your custom implementation to scikit-learn’s logistic regression model. By fitting scikit-learn’s version on the same data, you examine how closely the learned coefficients match those produced by your own class. This validates the correctness of your implementation. The lab concludes with a performance comparison using %timeit, revealing differences in computation speed between the two approaches and highlighting the practical considerations of implementing machine learning algorithms manually versus using optimized library tools.
